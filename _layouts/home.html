---
layout: single
toc: true
---
<h3>Abstract</h3>
<p>
  <blockquote>Pretrained language models, namely BERT, have shown large improvements in the
    commonsense
    reasoning
    benchmark COPA.
    However, recent work found that many improvements in benchmarks of natural language
    understanding
    are not due to models
    learning the task, but due to their increasing ability to exploit superficial cues, such as
    tokens
    that occur more often
    in the correct answer than the wrong one. Is BERT's good performance on COPA also caused by
    this?
    We find superficial cues in COPA, as well as evidence that BERT exploits these cues.
    To remedy this problem, we introduce Balanced COPA, an extension of COPA that does not suffer
    from
    easy-to-exploit
    single token cues.
    We analyze BERT's performance both on original and Balanced COPA, finding that BERT relies on
    superficial cues when they
    are present, but still achieves comparable performance once they are made ineffective,
    suggesting
    that BERT learns the
    task to a certain degree when forced to.
  </blockquote>
</p>