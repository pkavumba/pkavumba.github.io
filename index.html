<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: October 3, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.042e26407c9e383d96a1f26d6787c686.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=google-site-verification content="OFPhlIz_R75kjlev0PBr0xAK6PJN76rJUXnKDZRwgMo"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-68268899-5"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","UA-68268899-5",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Pride Kavumba"><meta name=description content="A PhD candidate at Tohoku University."><link rel=alternate hreflang=en-us href=https://pkavumba.github.io/><link rel=canonical href=https://pkavumba.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@pkavumba"><meta property="twitter:creator" content="@pkavumba"><meta property="twitter:image" content="https://pkavumba.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Pride Kavumba"><meta property="og:url" content="https://pkavumba.github.io/"><meta property="og:title" content="Pride Kavumba"><meta property="og:description" content="A PhD candidate at Tohoku University."><meta property="og:image" content="https://pkavumba.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-08-10T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://pkavumba.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://pkavumba.github.io/"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Pride Kavumba"><title>Pride Kavumba</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper data-wc-page-id=3976528693a0108357f4928017600865><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Pride Kavumba</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Pride Kavumba</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#featured data-target=#featured><span>Publications</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/pkavumba data-toggle=tooltip data-placement=bottom title="Follow me on Twitter" target=_blank rel=noopener aria-label="Follow me on Twitter"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about-biography"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=/authors/admin/avatar_hu4e3c0c5b15fbd1dd68ebb20a6419de78_30931_270x270_fill_q75_lanczos_center.jpeg alt="Pride Kavumba"><div class=portrait-title><h2>Pride Kavumba</h2><h3>PhD Candidate in NLP</h3><h3><a href=https://www.tohoku.ac.jp/ target=_blank rel=noopener><span>Tohoku University</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=/#contact aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href=https://twitter.com/pkavumba target=_blank rel=noopener aria-label=twitter><i class="fab fa-twitter big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?user=vcEkKCEAAAAJ&amp;hl" target=_blank rel=noopener aria-label=graduation-cap><i class="fas fa-graduation-cap big-icon"></i></a></li><li><a href=https://github.com/pkavumba target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Biography</h1><div class=article-style><p style=text-align:justify>I am Pride Kavumba, a Ph.D. candidate at Tohoku University, Japan, specializing in Natural Language Processing. My research interests include evaluation, robustness, interpretability, knowledge bases, and common sense reasoning.</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class="ul-interests fa-ul mb-0"><li><i class="fa-li fa-solid fa-book"></i>
Natural Language Processing</li><li><i class="fa-li fa-solid fa-book"></i>
Evaluation</li><li><i class="fa-li fa-solid fa-book"></i>
Robustness and Interpretability</li><li><i class="fa-li fa-solid fa-book"></i>
Knowledge base and Common Sense Reasoning</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fa-solid fa-graduation-cap"></i><div class=description><p class=course>PhD in NLP, 2023</p><p class=institution>Tohoku University</p></div></li><li><i class="fa-li fa-solid fa-graduation-cap"></i><div class=description><p class=course>Master of Information Science (NLP), 2020</p><p class=institution>Tohoku University</p></div></li><li><i class="fa-li fa-solid fa-graduation-cap"></i><div class=description><p class=course>BEng in Electrical Engineering and Electronics Engineering, 2013</p><p class=institution>The University of Zambia</p></div></li></ul></div></div></div></div></div></section><section id=projects class="home-section wg-portfolio"><div class=home-section-bg></div><div class=container><div class="row justify-content-center"><div class="section-heading col-12 mb-3 text-center"><h1 class=mb-0>Projects</h1></div><div class=col-12><div class="row js-layout-row project-showcase"><div class="col-12 isotope-item js-id-nlp"><div class="col-lg-12 mb-5 view-showcase"><div class="row align-items-center"><div class="col-12 col-md-6"><div class="section-subheading article-title mb-0 mt-0"><a href=https://github.com/pkavumba/scue>scue</a></div><div class=article-style>A unified framework for analyzing superficial cues in datasets.</div><div class=btn-links><a class="btn btn-outline-primary btn-page-header" href=https://github.com/pkavumba/scue target=_blank rel=noopener>Code</a></div></div><div class="col-12 col-md-6 order-first"><a href=https://github.com/pkavumba/scue><img src=/project/scue/featured_hu2a8a4d3a74b93e12e492a493736ca7dd_59055_540x0_resize_q75_h2_lanczos_3.webp height=405 width=540 alt=scue loading=lazy></a></div></div></div></div></div></div></div></div></section><section id=featured class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Conference Publications</h1></div><div class="col-12 col-lg-8"><div class="card-simple view-card"><div class=article-metadata><div><span>Pride Kavumba</span>, <span>Ana Brassard</span>, <span>Benjamin Heinzerling</span>, <span>Kentaro Inui</span></div><span class=article-date>May, 2023</span>
<span class=middot-divider></span>
<span class=pub-publication><em>Findings of the Association for Computational Linguistics: EACL 2023</em></span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/kavumba-etal-2023-prompting/>Prompting for explanations improves Adversarial NLI. Is this true? Yes it is true because it weakens superficial cues</a></div><a href=/publication/kavumba-etal-2023-prompting/ class=summary-link><div class=article-style><p>Explanation prompts ask language models to not only assign a particular label to a giveninput, such as true, entailment, or contradiction in the case of natural language inference but also to generate a free-text explanation that supports this label. For example: ``This is label because explanation.″ While this type of prompt was originally introduced with the aim of improving model interpretability, we showhere that explanation prompts also improve robustness to adversarial perturbations in naturallanguage inference benchmarks. Compared to prompting for labels only, explanation prompting consistently yields stronger performance on adversarial benchmarks, outperforming the state of the art on Adversarial Natural Language Inference, Counterfactually-Augmented Natural Language Inference, and SNLI-Hard datasets. We argue that the increase in robustness is due to the fact that prompting for explanations weakens superficial cues. Specifically, single tokens that are highly predictive of the correct answer in the label-only setting become uninformative when the model also has to generate explanations.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2023.findings-eacl.162.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kavumba-etal-2023-prompting/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2023.findings-eacl.162 target=_blank rel=noopener>URL</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Ana Brassard</span>, <span>Benjamin Heinzerling</span>, <span>Pride Kavumba</span>, <span>Kentaro Inui</span></div><span class=article-date>June, 2022</span>
<span class=middot-divider></span>
<span class=pub-publication><em>Proceedings of the Thirteenth Language Resources and Evaluation Conference</em></span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/brassard-etal-2022-copa/>COPA-SSE: Semi-structured Explanations for Commonsense Reasoning</a></div><a href=/publication/brassard-etal-2022-copa/ class=summary-link><div class=article-style><p>We present Semi-Structured Explanations for COPA (COPA-SSE), a new crowdsourced dataset of 9,747 semi-structured, English common sense explanations for Choice of Plausible Alternatives (COPA) questions. The explanations are formatted as a set of triple-like common sense statements with ConceptNet relations but freely written concepts. This semi-structured format strikes a balance between the high quality but low coverage of structured data and the lower quality but high coverage of free-form crowdsourcing. Each explanation also includes a set of human-given quality ratings. With their familiar format, the explanations are geared towards commonsense reasoners operating on knowledge graphs and serve as a starting point for ongoing work on improving such systems. The dataset is available at <a href=https://github.com/a-brassard/copa-sse target=_blank rel=noopener>https://github.com/a-brassard/copa-sse</a>.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.lrec-1.425.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/brassard-etal-2022-copa/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/a-brassard/copa-sse target=_blank rel=noopener>Dataset</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Pride Kavumba</span>, <span>Ryo Takahashi</span>, <span>Yusuke Oda</span></div><span class=article-date>May, 2022</span>
<span class=middot-divider></span>
<span class=pub-publication><em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em></span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/kavumba-etal-2022-prompt/>Are Prompt-based Models Clueless?</a></div><a href=/publication/kavumba-etal-2022-prompt/ class=summary-link><div class=article-style><p>Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks. However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets.Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective. Therefore, it is expected that few-shot prompt-based models do not exploit superficial cues.This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues.Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-long.166.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kavumba-etal-2022-prompt/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2022.acl-long.166 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-long.166 target=_blank rel=noopener>URL</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Pride Kavumba</span>, <span>Benjamin Heinzerling</span>, <span>Ana Brassard</span>, <span>Kentaro Inui</span></div><span class=article-date>June, 2021</span>
<span class=middot-divider></span>
<span class=pub-publication><em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em></span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/kavumba-etal-2021-learning/>Learning to Learn to be Right for the Right Reasons</a></div><a href=/publication/kavumba-etal-2021-learning/ class=summary-link><div class=article-style><p>Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the methods have improved performance on hard instances, they also lead to degraded performance on easy in- stances. Here, we propose to explicitly learn a model that does well on both the easy test set with superficial cues and the hard test set without superficial cues. Using a meta-learning objective, we learn such a model that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2021.naacl-main.304.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kavumba-etal-2021-learning/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2021.naacl-main.304 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2021.naacl-main.304 target=_blank rel=noopener>URL</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Keshav Singh</span>, <span>Paul Reisert</span>, <span>Naoya Inoue</span>, <span>Pride Kavumba</span>, <span>Kentaro Inui</span></div><span class=article-date>November, 2019</span>
<span class=middot-divider></span>
<span class=pub-publication><em>Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)</em></span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/singh-etal-2019-improving/>Improving Evidence Detection by Leveraging Warrants</a></div><a href=/publication/singh-etal-2019-improving/ class=summary-link><div class=article-style><p>Recognizing the implicit link between a claim and a piece of evidence (i.e. warrant) is the key to improving the performance of evidence detection. In this work, we explore the effectiveness of automatically extracted warrants for evidence detection. Given a claim and candidate evidence, our proposed method extracts multiple warrants via similarity search from an existing, structured corpus of arguments. We then attentively aggregate the extracted warrants, considering the consistency between the given argument and the acquired warrants. Although a qualitative analysis on the warrants shows that the extraction method needs to be improved, our results indicate that our method can still improve the performance of evidence detection.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/D19-6610.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/singh-etal-2019-improving/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/D19-6610 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/D19-6610 target=_blank rel=noopener>URL</a></div></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=section-collection class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Other Publications</h1></div><div class="col-12 col-lg-8"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Pride Kavumba</span>, <span>Ana Brassard</span>, <span>Benjamin Heinzerling</span>, <span>坂口慶祐</span>, <span>乾健太郎</span></span>
(2023).
<a href=/publication/kavumba-anlp-2023/>因果的 プロンプトによる NLI の敵対的ロバスト性の強化</a>.
<em>言語処理学会第 29 回年次大会</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/H9-3.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kavumba-anlp-2023/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Pride Kavumba</span>, <span>高橋諒</span>, <span>小田悠介</span></span>
(2022).
<a href=/publication/kavumba-anlp-2022/>プロンプトモデルは表面的手がかりを 利用するか</a>.
<em>言語処理学会第 28 回年次大会</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/E2-3.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kavumba-anlp-2022/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Pride Kavumba</span>, <span>Ana Brassard</span>, <span>Benjamin Heinzerling</span>, <span>Naoya Inoue</span>, <span>Kentaro Inui</span></span>
(2021).
<a href=/publication/kavumba-anlp-2021/>None the wiser? Adding “None’’Mitigates Superficial Cues in Multiple-Choice Benchmarks</a>.
<em>言語処理学会第 27 回年次大会</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.anlp.jp/proceedings/annual_meeting/2021/pdf_dir/E1-3.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kavumba-anlp-2021/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Pride Kavumba</span>, <span>Naoya Inoue</span>, <span>Kentaro Inui</span></span>
(2019).
<a href=/publication/kavumba-anlp-2019/>Exploring Supervised Learning of Hierarchical Event Embedding with Poincaré Embeddings</a>.
<em>言語処理学会第 25 回年次大会</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.anlp.jp/proceedings/annual_meeting/2019/pdf_dir/A3-2.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kavumba-anlp-2019/cite.bib>Cite</a></p></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Me.</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script src=https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.ead4a376bcf2c4714888c770ea9b0040.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>