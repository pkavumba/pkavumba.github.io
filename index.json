[{"authors":null,"categories":null,"content":"I am Pride Kavumba, a Ph.D. candidate at Tohoku University, Japan, specializing in Natural Language Processing. My research interests include evaluation, robustness, interpretability, knowledge bases, and common sense reasoning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am Pride Kavumba, a Ph.D. candidate at Tohoku University, Japan, specializing in Natural Language Processing. My research interests include evaluation, robustness, interpretability, knowledge bases, and common sense reasoning.","tags":null,"title":"Pride Kavumba","type":"authors"},{"authors":null,"categories":null,"content":"scue can reveal a dataset thats suspectable to the Clever Hans Effect. It can be used to analyze the effect of superficial cues in a dataset.\n","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"ea746574b002d994e7757be1acda0af0","permalink":"https://pkavumba.github.io/project/scue/","publishdate":"2023-07-10T00:00:00Z","relpermalink":"/project/scue/","section":"project","summary":"A unified framework for analyzing superficial cues in datasets.","tags":["NLP"],"title":"scue","type":"project"},{"authors":["Pride Kavumba","Ana Brassard","Benjamin Heinzerling","Kentaro Inui"],"categories":[],"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691854253,"objectID":"9e4e13a82f627d8a915a0d589d93b027","permalink":"https://pkavumba.github.io/publication/kavumba-etal-2023-prompting/","publishdate":"2023-08-12T15:30:53.045144Z","relpermalink":"/publication/kavumba-etal-2023-prompting/","section":"publication","summary":"Explanation prompts ask language models to not only assign a particular label to a giveninput, such as true, entailment, or contradiction in the case of natural language inference but also to generate a free-text explanation that supports this label. For example: ``This is label because explanation.″ While this type of prompt was originally introduced with the aim of improving model interpretability, we showhere that explanation prompts also improve robustness to adversarial perturbations in naturallanguage inference benchmarks. Compared to prompting for labels only, explanation prompting consistently yields stronger performance on adversarial benchmarks, outperforming the state of the art on Adversarial Natural Language Inference, Counterfactually-Augmented Natural Language Inference, and SNLI-Hard datasets. We argue that the increase in robustness is due to the fact that prompting for explanations weakens superficial cues. Specifically, single tokens that are highly predictive of the correct answer in the label-only setting become uninformative when the model also has to generate explanations.","tags":[],"title":"Prompting for explanations improves Adversarial NLI. Is this true? Yes it is true because it weakens superficial cues","type":"publication"},{"authors":["Pride Kavumba","Ana Brassard","Benjamin Heinzerling"," 坂口慶祐"," 乾健太郎"],"categories":[],"content":"","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691855431,"objectID":"83fb4ec6c0fc518316b4536f9889e940","permalink":"https://pkavumba.github.io/publication/kavumba-anlp-2023/","publishdate":"2023-08-12T15:50:30.837344Z","relpermalink":"/publication/kavumba-anlp-2023/","section":"publication","summary":"","tags":[],"title":"因果的 プロンプトによる NLI の敵対的ロバスト性の強化","type":"publication"},{"authors":["Ana Brassard","Benjamin Heinzerling","Pride Kavumba","Kentaro Inui"],"categories":[],"content":"","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691854253,"objectID":"64896b99ee3f3434f08d0f61caf73b6a","permalink":"https://pkavumba.github.io/publication/brassard-etal-2022-copa/","publishdate":"2023-08-12T15:30:53.287888Z","relpermalink":"/publication/brassard-etal-2022-copa/","section":"publication","summary":"We present Semi-Structured Explanations for COPA (COPA-SSE), a new crowdsourced dataset of 9,747 semi-structured, English common sense explanations for Choice of Plausible Alternatives (COPA) questions. The explanations are formatted as a set of triple-like common sense statements with ConceptNet relations but freely written concepts. This semi-structured format strikes a balance between the high quality but low coverage of structured data and the lower quality but high coverage of free-form crowdsourcing. Each explanation also includes a set of human-given quality ratings. With their familiar format, the explanations are geared towards commonsense reasoners operating on knowledge graphs and serve as a starting point for ongoing work on improving such systems. The dataset is available at https://github.com/a-brassard/copa-sse.","tags":[],"title":"COPA-SSE: Semi-structured Explanations for Commonsense Reasoning","type":"publication"},{"authors":["Pride Kavumba","Ryo Takahashi","Yusuke Oda"],"categories":[],"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691854253,"objectID":"e28d62ed1afcd8df2996d5fe4d0d69c6","permalink":"https://pkavumba.github.io/publication/kavumba-etal-2022-prompt/","publishdate":"2023-08-12T15:30:53.523609Z","relpermalink":"/publication/kavumba-etal-2022-prompt/","section":"publication","summary":"Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks. However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets.Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective. Therefore, it is expected that few-shot prompt-based models do not exploit superficial cues.This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues.Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues.","tags":[],"title":"Are Prompt-based Models Clueless?","type":"publication"},{"authors":["Pride Kavumba"," 高橋諒"," 小田悠介"],"categories":[],"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691855431,"objectID":"187275cdeab9102a62b921fb2dbdfcb3","permalink":"https://pkavumba.github.io/publication/kavumba-anlp-2022/","publishdate":"2023-08-12T15:50:31.114184Z","relpermalink":"/publication/kavumba-anlp-2022/","section":"publication","summary":"","tags":[],"title":"プロンプトモデルは表面的手がかりを 利用するか","type":"publication"},{"authors":["Pride Kavumba","Benjamin Heinzerling","Ana Brassard","Kentaro Inui"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691854253,"objectID":"d6a4ce15b3aa9398ea8552e5dff43410","permalink":"https://pkavumba.github.io/publication/kavumba-etal-2021-learning/","publishdate":"2023-08-12T15:30:53.750016Z","relpermalink":"/publication/kavumba-etal-2021-learning/","section":"publication","summary":"Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the methods have improved performance on hard instances, they also lead to degraded performance on easy in- stances. Here, we propose to explicitly learn a model that does well on both the easy test set with superficial cues and the hard test set without superficial cues. Using a meta-learning objective, we learn such a model that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline.","tags":[],"title":"Learning to Learn to be Right for the Right Reasons","type":"publication"},{"authors":["Pride Kavumba","Ana Brassard","Benjamin Heinzerling","Naoya Inoue","Kentaro Inui"],"categories":[],"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691855431,"objectID":"01982ef1b56503cfd4cbdad098e1f9fc","permalink":"https://pkavumba.github.io/publication/kavumba-anlp-2021/","publishdate":"2023-08-12T15:50:31.343116Z","relpermalink":"/publication/kavumba-anlp-2021/","section":"publication","summary":"","tags":[],"title":"None the wiser? Adding “None’’Mitigates Superficial Cues in Multiple-Choice Benchmarks","type":"publication"},{"authors":["Keshav Singh","Paul Reisert","Naoya Inoue","Pride Kavumba","Kentaro Inui"],"categories":[],"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691854254,"objectID":"c1184612436609ce83537dcf109f8236","permalink":"https://pkavumba.github.io/publication/singh-etal-2019-improving/","publishdate":"2023-08-12T15:30:54.199355Z","relpermalink":"/publication/singh-etal-2019-improving/","section":"publication","summary":"Recognizing the implicit link between a claim and a piece of evidence (i.e. warrant) is the key to improving the performance of evidence detection. In this work, we explore the effectiveness of automatically extracted warrants for evidence detection. Given a claim and candidate evidence, our proposed method extracts multiple warrants via similarity search from an existing, structured corpus of arguments. We then attentively aggregate the extracted warrants, considering the consistency between the given argument and the acquired warrants. Although a qualitative analysis on the warrants shows that the extraction method needs to be improved, our results indicate that our method can still improve the performance of evidence detection.","tags":[],"title":"Improving Evidence Detection by Leveraging Warrants","type":"publication"},{"authors":["Pride Kavumba","Naoya Inoue","Benjamin Heinzerling","Keshav Singh","Paul Reisert","Kentaro Inui"],"categories":[],"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691854254,"objectID":"9b85cbbc066755f7d98184e27598c444","permalink":"https://pkavumba.github.io/publication/kavumba-etal-2019-choosing/","publishdate":"2023-08-12T15:30:53.974456Z","relpermalink":"/publication/kavumba-etal-2019-choosing/","section":"publication","summary":"Pretrained language models, such as BERT and RoBERTa, have shown large improvements in the commonsense reasoning benchmark COPA. However, recent work found that many improvements in benchmarks of natural language understanding are not due to models learning the task, but due to their increasing ability to exploit superficial cues, such as tokens that occur more often in the correct answer than the wrong one. Are BERT′s and RoBERTa′s good performance on COPA also caused by this? We find superficial cues in COPA, as well as evidence that BERT exploits these cues.To remedy this problem, we introduce Balanced COPA, an extension of COPA that does not suffer from easy-to-exploit single token cues. We analyze BERT′s and RoBERTa′s performance on original and Balanced COPA, finding that BERT relies on superficial cues when they are present, but still achieves comparable performance once they are made ineffective, suggesting that BERT learns the task to a certain degree when forced to. In contrast, RoBERTa does not appear to rely on superficial cues.","tags":[],"title":"When Choosing Plausible Alternatives, Clever Hans can be Clever","type":"publication"},{"authors":["Pride Kavumba","Naoya Inoue","Kentaro Inui"],"categories":[],"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691855431,"objectID":"7b9efcafcfccaa89974ce561aa49c4a2","permalink":"https://pkavumba.github.io/publication/kavumba-anlp-2019/","publishdate":"2023-08-12T15:50:31.575685Z","relpermalink":"/publication/kavumba-anlp-2019/","section":"publication","summary":"","tags":[],"title":"Exploring Supervised Learning of Hierarchical Event Embedding with Poincaré Embeddings","type":"publication"}]