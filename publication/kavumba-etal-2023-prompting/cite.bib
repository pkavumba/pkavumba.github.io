@inproceedings{kavumba-etal-2023-prompting,
 abstract = {Explanation prompts ask language models to not only assign a particular label to a giveninput, such as true, entailment, or contradiction in the case of natural language inference but also to generate a free-text explanation that supports this label. For example: ``This is label because explanation.â€³ While this type of prompt was originally introduced with the aim of improving model interpretability, we showhere that explanation prompts also improve robustness to adversarial perturbations in naturallanguage inference benchmarks. Compared to prompting for labels only, explanation prompting consistently yields stronger performance on adversarial benchmarks, outperforming the state of the art on Adversarial Natural Language Inference, Counterfactually-Augmented Natural Language Inference, and SNLI-Hard datasets. We argue that the increase in robustness is due to the fact that prompting for explanations weakens superficial cues. Specifically, single tokens that are highly predictive of the correct answer in the label-only setting become uninformative when the model also has to generate explanations.},
 address = {Dubrovnik, Croatia},
 author = {Kavumba, Pride  and
Brassard, Ana  and
Heinzerling, Benjamin  and
Inui, Kentaro},
 booktitle = {Findings of the Association for Computational Linguistics: EACL 2023},
 month = {May},
 pages = {2165--2180},
 publisher = {Association for Computational Linguistics},
 title = {Prompting for explanations improves Adversarial NLI. Is this true? Yes it is true because it weakens superficial cues},
 url = {https://aclanthology.org/2023.findings-eacl.162},
 year = {2023}
}

